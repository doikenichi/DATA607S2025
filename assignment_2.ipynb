{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6a580c",
   "metadata": {},
   "source": [
    "# DATA 691 - Summer 2025 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962c682",
   "metadata": {},
   "source": [
    "### 1. Fashion MNIST\n",
    "\n",
    "Download the [Fashion MNIST dataset](https://www.kaggle.com/datasets/zalando-research/fashionmnist) from Kaggle.\n",
    "\n",
    "Load the 60,000 training examples (`fashion-mnist_train.csv`) and split it into equal size ***training*** and ***validation*** subsets.\n",
    "\n",
    "Your goal is to fit the best model you can to the training subset of size 30,000 constructed above, where best means highest accuracy on the validation set of size 30,000 constructed above.\n",
    "\n",
    "Your models should be pipelines with constituents chosen among the following:\n",
    "\n",
    "- `sklearn.preprocessing.StandardScaler`\n",
    "\n",
    "- `sklearn.preprocessing.Normalizer`\n",
    "\n",
    "- `sklearn.decomposition.PCA`\n",
    "\n",
    "- `sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "- `sklearn.svm.SVC`\n",
    "\n",
    "You don't have to use all of them!\n",
    "\n",
    "Restrict your parameter tuning to the the regularization parameter `C` of `LogisticRegression`/`SVC` and the `n_components` parameter of `PCA`.\n",
    "\n",
    "I suggest starting your analysis with a small subset of the training data, growing it as you identify good combinations of pipeline constituents and parameters.\n",
    "\n",
    "Once you have settled on an optimal model, evaluate it on the test set (`fashion-mnist_test.csv`) and report the corresponding accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the labels.\n",
    "\n",
    "labels = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f492a1d",
   "metadata": {},
   "source": [
    "### 2. SMS spam filtering\n",
    "\n",
    "In this activity, we'll try to flag spam SMS messages based on the text of the message.\n",
    "\n",
    "The dataset comes from the UCI Machine Learning Repository ([link](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)).\n",
    "\n",
    "I preprocessed it a bit. Load it from `data/sms_spam.csv`.\n",
    "\n",
    "#### Manual feature extraction\n",
    "\n",
    "If you examine some of the messages in the dataset, you'll notice some patterns that you might exploit for classifying spam messages. \n",
    "\n",
    "Add the following features to the dataframe:\n",
    "\n",
    "- `length`, the length of a message, in characters,\n",
    "\n",
    "- `num_caps`, the number of capital letters in a message,\n",
    "\n",
    "- `proportion_caps` the proportion of capital letters in a message,\n",
    "\n",
    "- `num_digits`, the number of digits in a message,\n",
    "\n",
    "- `proportion_digits` the proportion of digits in a message,\n",
    "\n",
    "- binary features `contains_<char>` indicating whether each of the following characters occurs in a message:\n",
    "`@`, `#`, `$`, `*`, `/`, `:`, `-`, `+`, `Â£`, `(`, `)`, `[`, `]`, `;`, `<`, `>`, `?`\n",
    "\n",
    "Compute cross-validated accuracy, $F_1$, precision, and recall metrics for `LogisticRegression`, `SGDClassifier`, and `LinearSVC` models fit to the the data using these features that you extracted.\n",
    "If you do this using `cross_val_score`, you'll need to loop over the metrics yourself. If you use [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) instead (try it!) you can pass in a list of metrics.\n",
    "\n",
    "How do the models compare across the various metrics?\n",
    "\n",
    "#### Feature extraction with `CountVectorizer`\n",
    "\n",
    "Compute cross-validated accuracy, $F_1$, precision, and recall metrics for `LogisticRegression`, `SGDClassifier`, `LinearSVC`, and `MultinomialNB` models fit to the data, extracting features using `CountVectorizer`.\n",
    "How do the models compare across the various metrics?\n",
    "\n",
    "Can you improve performance by tuning parameters of the vectorizer or the classifier? Does swapping out `CountVectorizer` with `TfidfVectorizer` improve any metrics?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data607s2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
